# ğŸ—ï¸ Data Engineering Architecture Documentation

## ğŸ“‹ Table of Contents
1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Technology Stack](#technology-stack)
4. [Data Flow](#data-flow)
5. [Component Details](#component-details)
6. [Database Schema](#database-schema)
7. [Kafka Implementation](#kafka-implementation)
8. [Monitoring & Testing](#monitoring--testing)
9. [Development Environment](#development-environment)
10. [Next Steps: ML & Fraud Detection](#next-steps-ml--fraud-detection)

---

## ğŸ¯ Project Overview

### Purpose
This project implements a **real-time data engineering pipeline** for processing sales transactions, with capabilities for:
- Real-time message streaming with Apache Kafka
- Multi-database persistence (MySQL, PostgreSQL, H2)
- Stream processing and analytics
- Foundation for ML-based fraud detection

### Business Use Case
**E-commerce Sales Processing System** that:
- Ingests sales transactions in real-time
- Processes and validates transaction data
- Stores data for analytics and reporting
- Provides foundation for fraud detection algorithms

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA ENGINEERING ARCHITECTURE                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources   â”‚    â”‚   Message Queue   â”‚    â”‚   Processing    â”‚
â”‚                  â”‚    â”‚                   â”‚    â”‚                 â”‚
â”‚ â€¢ CSV Files      â”‚â”€â”€â”€â–¶â”‚   Apache Kafka    â”‚â”€â”€â”€â–¶â”‚ â€¢ Producers     â”‚
â”‚ â€¢ APIs           â”‚    â”‚ â€¢ Sales Events    â”‚    â”‚ â€¢ Consumers     â”‚
â”‚ â€¢ Real-time      â”‚    â”‚ â€¢ High Value      â”‚    â”‚ â€¢ Streams       â”‚
â”‚   Transactions   â”‚    â”‚   Alerts          â”‚    â”‚   Processing    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                         â”‚
                                 â”‚                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Monitoring     â”‚    â”‚     Storage       â”‚    â”‚   Analytics     â”‚
â”‚                  â”‚    â”‚                   â”‚    â”‚                 â”‚
â”‚ â€¢ Kafka UI       â”‚    â”‚ â€¢ MySQL DB        â”‚    â”‚ â€¢ ETL Pipeline  â”‚
â”‚ â€¢ Adminer        â”‚    â”‚ â€¢ PostgreSQL      â”‚    â”‚ â€¢ Reporting     â”‚
â”‚ â€¢ Java Monitor   â”‚    â”‚ â€¢ H2 (Testing)    â”‚    â”‚ â€¢ Data Analysis â”‚
â”‚ â€¢ Docker Stats   â”‚    â”‚ â€¢ Schema Design   â”‚    â”‚ â€¢ Visualization â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

### Core Technologies
| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Message Queue** | Apache Kafka | 2.8+ | Real-time data streaming |
| **Stream Processing** | Kafka Streams | 2.8+ | Real-time analytics |
| **Databases** | MySQL | 8.0 | Production data storage |
| | PostgreSQL | 15 | Analytics & reporting |
| | H2 | 2.1+ | Testing & development |
| **Application** | Java | 11+ | Core business logic |
| **Build Tool** | Maven | 3.6+ | Dependency management |
| **Containerization** | Docker | 20+ | Infrastructure management |

### Supporting Tools
| Tool | Purpose |
|------|---------|
| **Kafka UI** | Web-based Kafka monitoring |
| **Adminer** | Database administration |
| **Zookeeper** | Kafka coordination |
| **Schema Registry** | Schema management |

---

## ğŸ”„ Data Flow

### 1. Data Ingestion Flow
```
CSV Files â†’ Java ETL â†’ SalesRecord Objects â†’ Validation â†’ Database
```

### 2. Real-time Streaming Flow
```
Sales Events â†’ Kafka Producer â†’ Kafka Topic â†’ Kafka Consumer â†’ Database
                     â†“
              Kafka Streams â†’ Analytics â†’ Enriched Topics
```

### 3. Processing Pipeline
```
Raw Data â†’ Validation â†’ Transformation â†’ Enrichment â†’ Storage â†’ Analytics
```

---

## ğŸ§© Component Details

### ğŸ“Š Data Model
**SalesRecord** - Core data entity with:
- `transactionId` - Unique transaction identifier
- `customerId` - Customer reference
- `productName` - Product description
- `productCategory` - Product classification
- `quantity` - Number of items
- `unitPrice` - Price per item
- `totalAmount` - Total transaction value
- `saleDate` - Transaction timestamp
- `storeLocation` - Store identifier
- `salesPerson` - Sales representative

### ğŸ­ Java Application Components

#### Core Classes
```
src/main/java/com/dataengineering/
â”œâ”€â”€ DataEngineeringApplication.java     # Main application entry point
â”œâ”€â”€ model/
â”‚   â””â”€â”€ SalesRecord.java               # Core data model
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ DatabaseConfig.java           # Database connection management
â”‚   â””â”€â”€ KafkaConfig.java              # Kafka configuration
â”œâ”€â”€ database/
â”‚   â””â”€â”€ DatabaseOperations.java       # Database CRUD operations
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer/SalesEventProducer.java   # Message publishing
â”‚   â”œâ”€â”€ consumer/SalesEventConsumer.java   # Message processing
â”‚   â””â”€â”€ streams/SalesStreamProcessor.java  # Stream analytics
â””â”€â”€ service/
    â””â”€â”€ KafkaIntegrationService.java      # System orchestration
```

#### Key Features
- **ETL Pipeline**: Extract, Transform, Load operations
- **Data Validation**: Input validation and error handling
- **Batch Processing**: Efficient bulk data operations
- **Real-time Processing**: Stream-based analytics
- **Multi-database Support**: H2, MySQL, PostgreSQL

---

## ğŸ—„ï¸ Database Schema

### Sales Records Table
```sql
CREATE TABLE sales_records (
    transaction_id VARCHAR(50) PRIMARY KEY,
    customer_id VARCHAR(50) NOT NULL,
    product_name VARCHAR(200) NOT NULL,
    product_category VARCHAR(100),
    quantity INTEGER NOT NULL,
    unit_price DECIMAL(10,2) NOT NULL,
    total_amount DECIMAL(10,2) NOT NULL,
    sale_date TIMESTAMP NOT NULL,
    store_location VARCHAR(100),
    sales_person VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Performance Indexes
- `idx_sales_customer_id` - Customer-based queries
- `idx_sales_date` - Time-based analytics
- `idx_sales_category` - Category analysis
- `idx_sales_location` - Location-based reporting

### Database Connections
| Database | Host | Port | Schema | User |
|----------|------|------|--------|------|
| MySQL | localhost | 3306 | data_engineering | dataeng |
| PostgreSQL | localhost | 5432 | data_engineering | dataeng |
| H2 | in-memory | - | testdb | sa |

---

## ğŸš€ Kafka Implementation

### Topic Structure
```
sales-events          # Main transaction stream
high-value-sales     # Transactions > $1000
enriched-sales       # Stream-processed data
category-aggregates  # Product category analytics
regional-analytics   # Location-based metrics
```

### Producer Configuration
- **Reliability**: `acks=all`, `retries=Integer.MAX_VALUE`
- **Performance**: Batch processing, async sending
- **Serialization**: JSON with Jackson ObjectMapper
- **Error Handling**: Retry logic and dead letter patterns

### Consumer Configuration
- **Consumer Group**: `sales-processor-group`
- **Offset Management**: Manual commits for reliability
- **Processing**: Business logic + database persistence
- **Error Handling**: Validation and error logging

### Stream Processing Features
- **Real-time Filtering**: High-value transaction detection
- **Aggregations**: Category and regional analytics
- **Windowing**: 5-minute tumbling windows
- **Data Enrichment**: Metadata addition
- **Branch Processing**: Multiple output streams

---

## ğŸ“Š Monitoring & Testing

### Available Monitoring Tools
1. **Kafka UI** (http://localhost:9090)
   - Topic management
   - Message browsing
   - Consumer group monitoring

2. **Database Adminer** (http://localhost:8080)
   - Database administration
   - Query execution
   - Schema management

3. **Java Monitoring**
   - `KafkaMonitor.java` - Programmatic monitoring
   - Live cluster information
   - Consumer group status

### Testing Suite
```
â”œâ”€â”€ testing/
â”‚   â”œâ”€â”€ ProducerTest.java              # Producer functionality
â”‚   â”œâ”€â”€ ConsumerTest.java              # Consumer operations
â”‚   â”œâ”€â”€ EndToEndTest.java              # Complete pipeline
â”‚   â”œâ”€â”€ StreamProcessorTestSuite.java  # Stream processing
â”‚   â”œâ”€â”€ ProducerConsumerTestSuite.java # Integration tests
â”‚   â””â”€â”€ SimpleStreamDemo.java          # Interactive demo
```

### Test Scenarios
- **Unit Tests**: Individual component testing
- **Integration Tests**: End-to-end pipeline validation
- **Load Tests**: Performance under volume
- **Interactive Tests**: Real-time user interaction
- **Monitoring Tests**: System health verification

---

## ğŸ³ Development Environment

### Docker Infrastructure
```yaml
# Database Services
- MySQL (data_engineering_mysql:3306)
- PostgreSQL (data_engineering_postgres:5432)  
- Adminer (data_engineering_adminer:8080)

# Kafka Services
- Zookeeper (data_engineering_zookeeper:2181)
- Kafka Broker (data_engineering_kafka:9092)
- Schema Registry (data_engineering_schema_registry:8081)
- Kafka UI (data_engineering_kafka_ui:9090)
```

### Quick Start Commands
```bash
# Start databases
docker-compose up -d mysql postgres adminer

# Start Kafka ecosystem  
docker-compose -f docker-compose-kafka.yml up -d

# Run all SQL setup
./run-all-sql.sh

# Test connections
./test-database-connections.sh

# Java application
mvn compile exec:java -Dexec.mainClass="com.dataengineering.DataEngineeringApplication"
```

### Project Structure
```
learning_big_data/
â”œâ”€â”€ src/main/java/com/dataengineering/    # Java source code
â”œâ”€â”€ src/main/resources/                   # Configuration files
â”œâ”€â”€ init-scripts/                         # Database initialization
â”œâ”€â”€ docker-compose.yml                    # Database containers
â”œâ”€â”€ docker-compose-kafka.yml              # Kafka containers
â”œâ”€â”€ pom.xml                               # Maven dependencies
â”œâ”€â”€ *.sh                                  # Utility scripts
â””â”€â”€ *.md                                  # Documentation
```

---

## ğŸ¯ Current Capabilities

### âœ… Implemented Features
- [x] **Multi-database ETL Pipeline**
- [x] **Real-time Kafka Streaming**
- [x] **Stream Processing Analytics**
- [x] **Comprehensive Testing Suite**
- [x] **Monitoring & Administration**
- [x] **Docker Infrastructure**
- [x] **Performance Optimizations**
- [x] **Data Validation & Error Handling**

### ğŸ“Š Analytics Capabilities
- Revenue analysis and reporting
- Product category performance
- Customer behavior tracking
- Time-based trend analysis
- Geographic distribution analysis
- High-value transaction alerts

### ğŸ”§ Operational Features
- Graceful shutdown handling
- Connection pooling ready
- Batch processing optimization
- Real-time health monitoring
- Automated testing suites
- Development environment automation

---

## ğŸš€ Next Steps: ML & Fraud Detection

### Planned ML Architecture Addition
```
Current Architecture + Apache Spark ML Pipeline
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML-ENHANCED ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Kafka Streams â”€â”€â†’ Feature Engineering â”€â”€â†’ ML Model â”€â”€â†’ Fraud Alerts
     â†“                     â†“                   â†“            â†“
Historical Data â”€â”€â†’ Training Pipeline â”€â”€â†’ Model Storage â”€â”€â†’ Dashboard
```

### Star Schema for Fraud Detection
Will implement dimensional modeling with:
- **Fact Table**: `fraud_transactions` (transaction measures + risk scores)
- **Dimensions**: customers, products, stores, payment_methods, date, time
- **Features**: Amount anomalies, behavioral patterns, temporal patterns

### ML Pipeline Components
1. **Feature Engineering**: Real-time feature extraction from streams
2. **Model Training**: Spark MLlib for fraud detection models
3. **Real-time Scoring**: Live fraud probability calculation
4. **Alert System**: Automated fraud alert generation
5. **Model Management**: Version control and A/B testing

### Integration Points
- **Data Source**: Current Kafka streams + historical database
- **Feature Store**: Enriched features for ML training
- **Model Serving**: Real-time prediction API
- **Feedback Loop**: Model retraining based on outcomes

---

## ğŸ“ Summary

This architecture provides a **solid foundation** for enterprise-grade data engineering:

### ğŸ¯ **Strengths**
- **Scalable**: Kafka-based streaming handles high throughput
- **Reliable**: Multi-database redundancy and error handling
- **Testable**: Comprehensive testing and monitoring
- **Maintainable**: Clean code structure and documentation
- **Extensible**: Ready for ML and advanced analytics

### ğŸ”§ **Ready for Enhancement**
- **ML Integration**: Foundation prepared for Spark ML pipeline
- **Fraud Detection**: Schema and data flow designed for ML features
- **Advanced Analytics**: Stream processing ready for complex algorithms
- **Production Deployment**: Container-ready infrastructure

**The current architecture serves as a robust platform for implementing machine learning-based fraud detection while maintaining all existing capabilities.**

---

*Documentation Version: 1.0*  
*Last Updated: July 2025*  
*Next Phase: Apache Spark ML Integration for Fraud Detection* 